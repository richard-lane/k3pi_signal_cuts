k3pi_signal_cuts
====
Cut away (some) background from a sample of D0 -> K3pi events.

Recommended Use
---
Assuming you're a new(ish) student doing a $D^0\rightarrow K3\pi$ analysis, here's how I recommend you use this repo:
 - Clone it
 - Follow the instructions in this Readme and in [data/README.md](data/README.md) to download a small amount of data
 - run `create_dumps.py` to create pandas DataFrames for the variables of interest
 - run some of the scripts in the `scripts` dir to check everything works - you should be able to plot histograms of the
   training variables, of $\Delta M$ and $M(D^0)$ for both the signal and background samples.
 - Train the classifier (i haven't implemented this yet)
 - Run some of the cross checks that I haven't implemented either

At this point you will know how to do some signal cuts on a sample of data.
When it comes to the actual analysis it will be impractical to download all the data (it may be practicable to download
all the MC but it might not be), so I'll need to decide what to do about that
My current thinking is - don't need to train these cuts on the whole data sample obviously, and it shouldn't matter anyway
since the data here I'm using is outside of the mass window used for the mass fit.
So i can just train the classifier on a small-ish subset of the data, perhaps retraining again for each year and then apply
that to the whole data sample (inside the allowed mass window that we're considering for the mass fit) when it comes to doing the analysis.


Input
----
The scripts in `scripts/` and training code all use pandas DataFrames that can be generated by running the [create_dumps.py](create_dumps.py) script - these require you to have downloaded some data from ROOT files on lxplus and put them in the right place in the `data/` directory.

Some notes on what to do are in the `data/` [README](data/README.md) - but basically you just have to download some ROOT files.

Algorithm
----
Uses a simple `RandomForestClassifier` for now

Scripts
----
The `scripts/` dir contains python scripts that generally create plots giving information about the classifier.
All use the holdout (testing) sample.

The more important/interesting ones are:
 - Histograms of the variables before and after applying cuts `scripts/plot_cuts.py`
 - A ROC curve (TODO)
 - A study into the optimal cut to use to get the maximal signal significance `scripts/plot_signal_significance.py`
 - A calibration curve (see below) `scripts/plot_calibration_curve.py`
 - The cut efficiency as a function of time `scripts/plot_cut_efficiency.py`


### A note on calibration
Since we are using this classifier to perform cuts based on probabilitiy, we would like the classification probability
returned by the classifier to be literally interpretable as a probability. i.e. - if we give our classifier 100
events and it assigns them all as signal with a probability of 0.8, we would like 80 of them to be signal and 20 to be
background.
In general, an ensemble of trees will not return calibrated probabilities - this is for good reasons (all trees would
have to return 0 for the ensemble to report 0 probability, so the classifier is unlikely to be calibrated at the tails).

#### The thing I'm currently confused about
This probably doesn't really matter for us - we will be optimising the threshhold value by finding the maximum signal
significance - but the optimal cut here DOES depend on the relative amounts of signal/background used in the training
and testing samples.
 - should I train on a balanced dataset (i.e. same amount of signal + background)?
 - should I test on a balanced dataset, or one with some imbalance? The optimal cut will depend on the relative amount
   of signal and background in the testing sample - which I don't know for real data...
 - Does having an imbalance in testing/training datasets really cause issues with calibration..?

